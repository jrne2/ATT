# pages/learning.py
from dotenv import load_dotenv
load_dotenv()

from core.session import initialize_session, add_message
initialize_session()

import streamlit as st
from core.ai import get_ai_response, transcribe_audio, text_to_audio, get_hint
from streamlit_mic_recorder import mic_recorder
import core.feature_extractor as fe
import core.analyzer as an

# --- 1. ì‚¬ì´ë“œë°” UI êµ¬ì„± ---
with st.sidebar:
    st.header("âš™ï¸ í•™ìŠµ ì„¤ì •")
    st.write(f"**í˜ë¥´ì†Œë‚˜:** {st.session_state.persona}")
    user_level = st.radio("ë‹¹ì‹ ì˜ ìˆ˜ì¤€ì„ ì„ íƒí•˜ì„¸ìš”", ["ì´ˆë³´ì", "ì¤‘ê¸‰ì"], index=1, horizontal=True)
    
    language_options = {'ì˜ì–´': 'en-US'}
    selected_language_name = st.selectbox("í•™ìŠµí•  ì–¸ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”.", options=list(language_options.keys()))
    selected_language_code = language_options[selected_language_name]

    st.divider()

    st.header("ğŸ¤ ìŒì„±ìœ¼ë¡œ ëŒ€í™”í•˜ê¸°")
    audio_info = mic_recorder(start_prompt="ğŸ¤ ë…¹ìŒ ì‹œì‘", stop_prompt="â¹ï¸ ë…¹ìŒ ì¤‘ì§€", key='recorder')

    if audio_info:
        st.audio(audio_info['bytes'], format="audio/wav")
        if st.button("ìŒì„± ë¶„ì„í•˜ê¸°"):
            wav_bytes = audio_info['bytes']
            with st.spinner("AIê°€ ë¶„ì„ ì¤‘..."):
                user_prompt_from_audio = transcribe_audio(wav_bytes)

                if user_prompt_from_audio:
                    add_message("user", user_prompt_from_audio)
                    
                    complexity = fe.get_complexity_score(user_prompt_from_audio)
                    sentiment = fe.get_sentiment(user_prompt_from_audio)
                    
                    response, feedback, score = get_ai_response(st.session_state.persona, user_prompt_from_audio, learning_language=selected_language_name)
                    
                    st.session_state.messages[-1]['features'] = {'complexity': complexity, 'sentiment': sentiment, 'score': score}
                    
                    ai_audio_bytes = text_to_audio(response)
                    st.session_state.messages.append({"role": "assistant", "content": response, "feedback": feedback, "audio": ai_audio_bytes})
                    st.rerun()
                else:
                    st.error("ìŒì„±ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    
    st.divider()
    if st.button("ğŸ’¡ íŒíŠ¸ ë³´ì—¬ì£¼ê¸°"):
        with st.spinner("íŒíŠ¸ë¥¼ ìƒì„± ì¤‘..."):
            hint = get_hint(user_level, st.session_state.messages, learning_language=selected_language_name)
            st.info(hint)

    st.divider()
    st.page_link("pages/my.py", label="**í•™ìŠµ ì¢…ë£Œ ë° ê²°ê³¼ ë³´ê¸°**", icon="ğŸ“Š", use_container_width=True)


# --- 2. ë©”ì¸ ì±„íŒ… UI êµ¬ì„± ---
st.title("ğŸ’¬ í•™ìŠµí•˜ê¸°")
st.write("ì‚¬ì´ë“œë°”ì˜ ë§ˆì´í¬ë¥¼ ì´ìš©í•´ ìŒì„±ìœ¼ë¡œ ëŒ€í™”í•˜ê±°ë‚˜, ì•„ë˜ ì…ë ¥ì°½ì— í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "feedback" in message:
            st.info(message["feedback"])
        if "audio" in message and message.get("role") == "assistant":
            st.audio(message["audio"], format="audio/mp3", autoplay=True)

if user_prompt := st.chat_input("í…ìŠ¤íŠ¸ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ë³´ì„¸ìš”..."):
    add_message("user", user_prompt)
    with st.spinner("AIê°€ ë¶„ì„ ì¤‘..."):
        
        response, feedback, score = get_ai_response(
            st.session_state.persona, 
            user_prompt,
            learning_language=selected_language_name,
            feedback_language='Korean'
        )

        st.session_state.messages[-1]['features'] = {'score': score}
        
        ai_audio_bytes = text_to_audio(response)
        
        st.session_state.messages.append({
            "role": "assistant", 
            "content": response, 
            "feedback": feedback,
            "audio": ai_audio_bytes
        })
        st.rerun()